{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean_happiness_datasets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJXmpWuDQFFd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.core.display import display, HTML\n",
        "from dateutil.parser import parse\n",
        "\n",
        "# Goal: create dataset for 2013, 2018; clean data; create one big database containing all\n",
        "# importing the datasets csv previously scrapped and collected\n",
        "datasets = []\n",
        "df_10 = pd.read_csv('/content/US_Happiness_2010.csv')\n",
        "datasets.append(df_10)\n",
        "df_11 = pd.read_csv('/content/US_Happiness_2011.csv')\n",
        "datasets.append(df_11)\n",
        "df_12 = pd.read_csv('/content/US_Happiness_2012.csv')\n",
        "datasets.append(df_12)\n",
        "df_14 = pd.read_csv('/content/US_Happiness_2014.csv')\n",
        "datasets.append(df_14)\n",
        "df_15 = pd.read_csv('/content/US_Happiness_2015.csv')\n",
        "datasets.append(df_15)\n",
        "df_16 = pd.read_csv('/content/US_Happiness_2016.csv')\n",
        "datasets.append(df_16)\n",
        "df_17 = pd.read_csv('/content/US_Happiness_2017.csv')\n",
        "datasets.append(df_17)\n",
        "df_19 = pd.read_csv('/content/US_Happiness_2019.csv')\n",
        "datasets.append(df_19)\n",
        "df_20 = pd.read_csv('/content/US_Happiness_2020.csv')\n",
        "datasets.append(df_20)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW-OzF5psKm_"
      },
      "source": [
        "# rename states to their shortened names\n",
        "# dictionary that maps full names to shortened names\n",
        "state_dict = {\n",
        "    'Alabama': 'AL',\n",
        "    'Alaska': 'AK',\n",
        "    'American Samoa': 'AS',\n",
        "    'Arizona': 'AZ',\n",
        "    'Arkansas': 'AR',\n",
        "    'California': 'CA',\n",
        "    'Colorado': 'CO',\n",
        "    'Connecticut': 'CT',\n",
        "    'Delaware': 'DE',\n",
        "    'District of Columbia': 'DC',\n",
        "    'Florida': 'FL',\n",
        "    'Georgia': 'GA',\n",
        "    'Guam': 'GU',\n",
        "    'Hawaii': 'HI',\n",
        "    'Idaho': 'ID',\n",
        "    'Illinois': 'IL',\n",
        "    'Indiana': 'IN',\n",
        "    'Iowa': 'IA',\n",
        "    'Kansas': 'KS',\n",
        "    'Kentucky': 'KY',\n",
        "    'Louisiana': 'LA',\n",
        "    'Maine': 'ME',\n",
        "    'Maryland': 'MD',\n",
        "    'Massachusetts': 'MA',\n",
        "    'Michigan': 'MI',\n",
        "    'Minnesota': 'MN',\n",
        "    'Mississippi': 'MS',\n",
        "    'Missouri': 'MO',\n",
        "    'Montana': 'MT',\n",
        "    'Nebraska': 'NE',\n",
        "    'Nevada': 'NV',\n",
        "    'New Hampshire': 'NH',\n",
        "    'New Jersey': 'NJ',\n",
        "    'New Mexico': 'NM',\n",
        "    'New York': 'NY',\n",
        "    'North Carolina': 'NC',\n",
        "    'North Dakota': 'ND',\n",
        "    'Northern Mariana Islands':'MP',\n",
        "    'Ohio': 'OH',\n",
        "    'Oklahoma': 'OK',\n",
        "    'Oregon': 'OR',\n",
        "    'Pennsylvania': 'PA',\n",
        "    'Puerto Rico': 'PR',\n",
        "    'Rhode Island': 'RI',\n",
        "    'South Carolina': 'SC',\n",
        "    'South Dakota': 'SD',\n",
        "    'Tennessee': 'TN',\n",
        "    'Texas': 'TX',\n",
        "    'Utah': 'UT',\n",
        "    'Vermont': 'VT',\n",
        "    'Virgin Islands': 'VI',\n",
        "    'Virginia': 'VA',\n",
        "    'Washington': 'WA',\n",
        "    'West Virginia': 'WV',\n",
        "    'Wisconsin': 'WI',\n",
        "    'Wyoming': 'WY'\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjI7RrNebM9-"
      },
      "source": [
        "# cleaning all the datasets and editing them to one format\n",
        "# creating 2013 by calculating the average of 2012 and 2014\n",
        "\n",
        "df_13 = pd.concat([df_12, df_14]).groupby('State', as_index=False).mean()\n",
        "df_13 = df_13.sort_values(by=['Happiness Score'], ascending=False)\n",
        "#display(df_13)\n",
        "datasets.append(df_13)\n",
        "\n",
        "# 2019 requires name correction \n",
        "for i in df_19.index:\n",
        "      try:\n",
        "        df_19.loc[i, 'State'] = state_dict[df_19.loc[i, 'State']]\n",
        "      except:\n",
        "        print(df_19.loc[i, 'State'], 'not a name')\n",
        "\n",
        "# creating 2018 by by calculating the average of 2017 and 2019\n",
        "df_18 = pd.concat([df_17, df_19]).groupby('State', as_index=False).mean()\n",
        "df_18 = df_18.sort_values(by=['Happiness Score'], ascending=False)\n",
        "display(df_18)\n",
        "datasets.append(df_18)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rqd-7c9f3Et"
      },
      "source": [
        "# adding appropriate year column to each dataset\n",
        "y = ['2010', '2011', '2012', '2014', '2015', '2016', '2017', '2019', '2020', '2013', '2018']\n",
        "for e in range(len(datasets)):\n",
        "  year = []\n",
        "  for i in range(len(datasets[e])):\n",
        "    \n",
        "    year.append(y[e])\n",
        "\n",
        "  datasets[e]['Year'] = year\n",
        "  display(datasets[e])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HerdNy-jszHu"
      },
      "source": [
        "# edit names\n",
        "for df in datasets:\n",
        "  for i in df.index:\n",
        "      try:\n",
        "        df.loc[i, 'State'] = state_dict[df.loc[i, 'State']]\n",
        "      except:\n",
        "        print(df.loc[i, 'State'], 'not a name')\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83Gt-Om0uxOf"
      },
      "source": [
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "#display(datasets[10])\n",
        "print(datasets[10])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6DXvtASzrYT"
      },
      "source": [
        "# join all the dataframes into one mega dataframe\n",
        "frames = [df_10,df_11,df_12,df_13,df_14,df_15,df_16,df_17,df_18,df_19,df_20]\n",
        "result = pd.concat(frames, ignore_index=True)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd-47lF838Si"
      },
      "source": [
        "# create csv\n",
        "result.to_csv('Happiness_dataset.csv')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76AdqP2M1Kec"
      },
      "source": [
        "# testing\n",
        "a = pd.DataFrame(['a','b','c','d','e','f','g'], columns=['pog'])\n",
        "b = pd.DataFrame(['h','i','l','m','n','o','p'], columns=['pog'])\n",
        "\n",
        "asd = [a,b]\n",
        "\n",
        "c = pd.concat(asd,ignore_index=True)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}